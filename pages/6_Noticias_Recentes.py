import streamlit as st
import feedparser
import calendar
from datetime import datetime, timezone
from time import mktime # Converter a data do RSS
from urllib.parse import quote_plus # Formatar a query da URL
from zoneinfo import ZoneInfo
from bs4 import BeautifulSoup
from chatbot import chatbot_component

chatbot_component()

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="Not√≠cias Recentes - Alerta Metanol",
    page_icon="üì∞",
    layout="wide"
)

st.title("Not√≠cias Recentes sobre Metanol e Bebidas Adulteradas")
st.markdown("Mantenha-se informado com as √∫ltimas not√≠cias sobre os riscos, apreens√µes e casos relacionados.")
st.markdown("---")

# --- L√ìGICA PARA BUSCAR NOT√çCIAS ---

# Usamos o cache para n√£o chamar a API toda vez que a p√°gina recarrega
# O TTL (Time To Live) de 3600 segundos significa que os dados ser√£o atualizados a cada hora
@st.cache_data(ttl=3600)
def buscar_noticias():
    # Termos de busca. Usamos "OR" para encontrar qualquer um dos termos.
    queries = [
    '"metanol"',
    '"intoxica√ß√£o alco√≥lica"',
    '"bebida falsificada"',
    '"bebida adulterada"',
    '"apreens√£o de bebida"',
    '"√°lcool adulterado"',
    '"√°lcool falsificado"',
    '"intoxica√ß√£o por metanol"',
    '"envenenamento por metanol"',
    '"√°lcool ilegal"',
    '"√°lcool clandestino"'
    ]

    artigos_combinados = []
    links_vistos = set() # Usado para evitar duplicatas
    titulos_vistos = set()

    for query in queries:
    # Codificamos os termos para usar na URL do Google
        query_encoded = quote_plus(query)

        # URL do Feed RSS do Google Not√≠cias
        url = (
            f'https://news.google.com/rss/search?'
            f'q={query_encoded}&'
            'hl=pt-BR&'
            'gl=BR&'
            'ceid=BR:pt-419'
        )

        try:
            # Usamos o feedparser para "parsear" (ler) o XML do RSS
            feed = feedparser.parse(url)
            
            artigos_combinados = []
            
            # Iteramos pelas entradas do feed
            for entrada in feed.entries:
                nome_fonte = entrada.source.title if 'source' in entrada else 'Desconhecida'
                # Remover sufixo da fonte do t√≠tulo, se presente
                titulo_original = entrada.title
                
                sufixo_fonte = f" - {nome_fonte}"

                if titulo_original.endswith(sufixo_fonte):
                    entrada.title = titulo_original[:-len(sufixo_fonte)].strip()

                # Evita duplicatas baseadas em link ou t√≠tulo
                if entrada.link in links_vistos or entrada.title in titulos_vistos:
                    continue  # Pula duplicatas
                    
                # 'published_parsed' √© um struct_time em UTC (GMT)
                # 'calendar.timegm' converte um struct_time UTC para um timestamp UTC
                utc_timestamp = calendar.timegm(entrada.published_parsed)
                
                # Cria um objeto datetime "aware" (com fuso) em UTC
                dt_publicacao_utc = datetime.fromtimestamp(utc_timestamp, tz=timezone.utc)
                
                # Salva como string ISO. Ela agora ter√° a informa√ß√£o de fuso (+00:00)
                iso_data = dt_publicacao_utc.isoformat()

                # Montamos o dicion√°rio no mesmo formato que a News API fornecia
                artigos_combinados.append({
                    "title": entrada.title,
                    "description": entrada.summary, # O RSS chama de 'summary'
                    "url": entrada.link,
                    "urlToImage": None, # IMPORTANTE: RSS do Google n√£o fornece imagem
                    "publishedAt": iso_data,
                    "source": {"name": entrada.source.title} # Fonte da not√≠cia
                })
                links_vistos.add(entrada.link)
                titulos_vistos.add(entrada.title)
        except Exception as e:
            st.error(f"Erro ao buscar feed para a query '{query}': {e}")
            
        artigos_combinados.sort(key=lambda x: x["publishedAt"], reverse=True) # Ordena por data decrescente

        return artigos_combinados

# --- EXIBI√á√ÉO DAS NOT√çCIAS ---
ZONA_LOCAL = ZoneInfo("America/Fortaleza")  # Fuso hor√°rio da Para√≠ba, Brasil
artigos = buscar_noticias()

if artigos:
    # Limita a 15 not√≠cias para n√£o poluir a p√°gina
    for artigo in artigos[:15]:
        fonte = artigo["source"]["name"]
        
        dt_obj_utc = datetime.fromisoformat(artigo["publishedAt"])  # 1. L√™ a string ISO (que j√° tem o fuso +00:00) para um objeto datetime
        dt_obj_local = dt_obj_utc.astimezone(ZONA_LOCAL) # 2. Converte o objeto datetime de UTC para o fuso local (Paraiba)
        data_publicacao = dt_obj_local.strftime('%d/%m/%Y √†s %H:%M') # 3. Formata a data/hora local

        st.subheader(artigo["title"])
        st.caption(f"Fonte: {fonte} | Publicado em: {data_publicacao}")

        st.markdown(f"[Ler a mat√©ria completa ‚Üí]({artigo['url']})")

        st.markdown("---")
else:
    st.warning("Nenhuma not√≠cia recente encontrada sobre o tema.")
